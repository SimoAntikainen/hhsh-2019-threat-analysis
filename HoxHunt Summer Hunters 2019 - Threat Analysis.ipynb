{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoxHunt Summer Hunters 2019 - Data - Home assignment\n",
    "\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/zmuij2fyjo27j1u/Screenshot%202019-03-21%2017.28.10.png?dl=1\" width=\"1000\">\n",
    "\n",
    "## Assignment\n",
    "\n",
    "In this assignment you as a HoxHunt Data Science Hunter are given the task to extract interesting features from a possible malicious indicator of compromise, more specifically in this case from a given potentially malicious URL. \n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/ao0neaphtfama7g/Screenshot%202019-03-21%2017.23.40.png?dl=1\" width=\"400\">\n",
    "\n",
    "This assignment assumes that you are comfortable (or quick to learn) on using Jupyter Notebooks and suitable programming enviroment such as Python, R or Julia. The example below uses Python and has some external dependencies such as Requests library.\n",
    "\n",
    "Happy hunting!\n",
    "\n",
    "\n",
    "## Interesting research papers & resources\n",
    "\n",
    "Below is a list of interesting research papers on the topic. They might give you good tips what features you could extract from a given URL:\n",
    "\n",
    "\n",
    "[Know Your Phish: Novel Techniques for Detecting\n",
    "Phishing Sites and their Targets](https://arxiv.org/pdf/1510.06501.pdf)\n",
    "\n",
    "[DeltaPhish: Detecting Phishing Webpages\n",
    "in Compromised Websites](https://arxiv.org/pdf/1707.00317.pdf)\n",
    "\n",
    "[PhishAri: Automatic Realtime Phishing Detection on Twitter](https://arxiv.org/pdf/1301.6899.pdf)\n",
    "\n",
    "[More or Less? Predict the Social Influence of Malicious URLs on Social Media\n",
    "](https://arxiv.org/abs/1812.02978)\n",
    "\n",
    "[awesome-threat-intelligence](https://github.com/hslatman/awesome-threat-intelligence)\n",
    "\n",
    "\n",
    "\n",
    "## What we expect\n",
    "\n",
    "Investigate potential features you could extract from the given URL and implement extractors for the ones that interest you the most. Below example code extracts one feature but does not store it very efficiently (just console logs it). Implement sensible data structure using some known data structure library to store the features per URL. Also consider how would you approach error handling if one feature extractor fails?\n",
    "\n",
    "Be prepared to discuss questions such as: what features could indicate the malicousness of a given URL? What goes in to the thinking of the attacker when they are choosing a site for an attack? What would you develop next?\n",
    "\n",
    "## What we don't expect\n",
    "\n",
    "Implement a humangous set of features.\n",
    "\n",
    "Implement any kind of actual predicition models that uses the features to give predictions on malicousness at this stage :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "import tldextract\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.slideshare.net/weaveworks/client-side-monitoring-with-prometheus 4739\n",
      "http://cartaobndes.gov.br.cv31792.tmweb.ru/ 4653\n",
      "https://paypal.co.uk.yatn.eu/m/ None\n",
      "http://college-eisk.ru/cli/ 2722\n",
      "https://dotpay-platnosc3.eu/dotpay/ None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_domain_age_in_days(domain):\n",
    "    show = \"https://input.payapi.io/v1/api/fraud/domain/age/\" + domain\n",
    "    data = requests.get(show).json()\n",
    "    return data['result'] if 'result' in data else None\n",
    "\n",
    "def parse_domain_from_url(url):\n",
    "    t = urlparse(url).netloc\n",
    "    return '.'.join(t.split('.')[-2:])\n",
    "\n",
    "def analyze_url(url):\n",
    "    # First feature, if domain is new it could indicate that the bad guy has bought it recently...\n",
    "    age_in_days_feature = get_domain_age_in_days(parse_domain_from_url(url));\n",
    "    # Hmm...maybe I could do something more sensible with the data than just printing out\n",
    "    print(url, age_in_days_feature)\n",
    "\n",
    "# Note some of these urls are live phishing sites (as of 2019-03-21) use with caution! More can be found at https://www.phishtank.com/\n",
    "example_urls = [\"https://www.slideshare.net/weaveworks/client-side-monitoring-with-prometheus\",\n",
    "                \"http://cartaobndes.gov.br.cv31792.tmweb.ru/\",\n",
    "                \"https://paypal.co.uk.yatn.eu/m/\",\n",
    "                \"http://college-eisk.ru/cli/\",\n",
    "                \"https://dotpay-platnosc3.eu/dotpay/\"\n",
    "               ]\n",
    "for url in example_urls: \n",
    "    analyze_url(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 106 URL\n",
    "f2 66 Term usage consistency\n",
    "f3 22 Usage of starting and landing mld\n",
    "f4 13 RDN usage\n",
    "f5 5 Webpage content\n",
    "\n",
    "url related features had the best precision, recall and FP-rate, together with term use frequency, but a combination of url and webpage content related features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_urls = [\"https://www.slideshare.net/weaveworks/client-side-monitoring-with-prometheus\",\n",
    "                \"http://cartaobndes.gov.br.cv31792.tmweb.ru/\",\n",
    "                \"https://paypal.co.uk.yatn.eu/m/\",\n",
    "                \"http://college-eisk.ru/cli/\",\n",
    "                \"https://dotpay-platnosc3.eu/dotpay/\"\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.slideshare.net/weaveworks/client-side-monitoring-with-prometheus',\n",
       "  'https://www.slideshare.net/weaveworks/client-side-monitoring-with-prometheus'),\n",
       " ('http://cartaobndes.gov.br.cv31792.tmweb.ru/',\n",
       "  'https://vh76.timeweb.ru/parking/?ref=cartaobndes.gov.br.cv31792.tmweb.ru'),\n",
       " ('https://paypal.co.uk.yatn.eu/m/', None),\n",
       " ('http://college-eisk.ru/cli/', None),\n",
       " ('https://dotpay-platnosc3.eu/dotpay/', None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_land_urls = []\n",
    "for url in example_urls:\n",
    "    try:\n",
    "        response = urlopen(url)\n",
    "        landing_url = response.geturl()\n",
    "        start_land_urls.append((url, landing_url))\n",
    "    except:\n",
    "        start_land_urls.append((url, None))\n",
    "\n",
    "start_land_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https', 'www.slideshare.net', 'slideshare.net', 'slideshare', 'www', '/weaveworks/client-side-monitoring-with-prometheus')\n",
      "('http', 'cartaobndes.gov.br.cv31792.tmweb.ru', 'tmweb.ru', 'tmweb', 'cartaobndes.gov.br.cv31792', '/')\n",
      "('https', 'paypal.co.uk.yatn.eu', 'yatn.eu', 'yatn', 'paypal.co.uk', '/m/')\n",
      "('http', 'college-eisk.ru', 'college-eisk.ru', 'college-eisk', '', '/cli/')\n",
      "('https', 'dotpay-platnosc3.eu', 'dotpay-platnosc3.eu', 'dotpay-platnosc3', '', '/dotpay/')\n"
     ]
    }
   ],
   "source": [
    "def url_components(url):\n",
    "    url_parsed =  urlparse(url)\n",
    "    url_extracted = tldextract.extract(url)\n",
    "    \n",
    "    protocol = url_parsed.scheme\n",
    "    FQDN = url_parsed.netloc\n",
    "    RDN = url_extracted.domain + '.' + url_extracted.suffix\n",
    "    mld = url_extracted.domain\n",
    "    FreeURL_start = url_extracted.subdomain\n",
    "    FreeURL_end = url_parsed.path\n",
    "    \n",
    "    return (protocol,FQDN, RDN,mld,FreeURL_start, FreeURL_end)\n",
    "\n",
    "for url in example_urls:\n",
    "    print(url_components(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResult(subdomain='paypal.co.uk', domain='yatn', suffix='eu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url_parsed =  urlparse(example_urls[0])\n",
    "#test = url_parsed.hostname\n",
    "#print(url_parsed)\n",
    "tldextract.extract(example_urls[2])\n",
    "#tldextract.extract('https://www.amazon.co.uk/ap/signin? encoding=UTF8')\n",
    "#urlparse(example_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://cartaobndes.gov.br.cv31792.tmweb.ru/\n",
      "['https', 'paypal', 'co', 'uk', 'yatn', 'eu', 'm', '']\n"
     ]
    }
   ],
   "source": [
    "print(example_urls[1])\n",
    "url_terms = re.split(r\"[/:\\.?=&]+\",example_urls[2])\n",
    "print(url_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https', 0, 76, 18, 10, 10, 1) ('https', 0, 76, 18, 10, 10, 1)\n",
      "('http', 3, 43, 35, 5, 8, 1) ('https', 0, 72, 15, 7, 12, 1)\n",
      "('https', 2, 31, 20, 4, 8, 1) None\n",
      "('http', 0, 27, 15, 12, 6, 2) None\n",
      "('https', 0, 35, 19, 16, 6, 2) None\n"
     ]
    }
   ],
   "source": [
    "def url_features(url):\n",
    "    \n",
    "    if url != None:\n",
    "        \n",
    "        url_parsed =  urlparse(url)\n",
    "        # protocol used (http/https)\n",
    "        protocol,FQDN, RDN, mld, FreeURL_start, FreeURL_end = url_components(url)\n",
    "        #count of dots ‘.’ in FreeURL\n",
    "        free_url_dots = FreeURL_start.count('.') +  FreeURL_end.count('.')\n",
    "        \n",
    "        #Count of level domains \n",
    "        \n",
    "        #length of the URL\n",
    "        url_length = len(url)\n",
    "        # length of the FQDN\n",
    "        FQDN_length = len(FQDN)\n",
    "        #length of the mld\n",
    "        mld_length = len(mld)\n",
    "        #count of terms in the URL \n",
    "        url_terms = len(re.split(r\"[/:\\.?=&-_]+\",url))\n",
    "        #count of terms in the mld  \n",
    "        mld_terms = len(re.split(r\"[-]+\",mld))\n",
    " \n",
    "        return (protocol,free_url_dots, url_length,FQDN_length,mld_length,url_terms,mld_terms)\n",
    "\n",
    "    \n",
    "for url in start_land_urls: \n",
    "    print(url_features(url[0]), url_features(url[1]))\n",
    "#print(url_features(example_urls[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(example_urls[0])\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#artist_name_list = soup.find('body')\n",
    "#artist_name_list.string\n",
    "#nltk.word_tokenize(artist_name_list.string)\n",
    "#text = artist_name_list.getText()\n",
    "#nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "#html = urlopen(example_urls[0]).read()\n",
    "#text = text_from_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 414, 13, 50, 0)\n"
     ]
    }
   ],
   "source": [
    "def title_terms_amount(url):\n",
    "    \n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    title = soup.find('title').string\n",
    "    tokenized_title = nltk.word_tokenize(title)\n",
    "    long_terms = [x for x in tokenized_title if len(x) >= 3]\n",
    "    \n",
    "    return len(set(long_terms))\n",
    "\n",
    "def body_terms_amount(url):\n",
    "    \n",
    "    html = urlopen(url).read()\n",
    "    text = text_from_html(html)\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    long_terms = [x for x in tokenized_text if len(x) >= 3]\n",
    "    \n",
    "    return len(set(long_terms))\n",
    "\n",
    "def input_field_amount(url):\n",
    "    #the research paper does not discern between types of input fields\n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    input_fields = soup.find_all('input')\n",
    "    \n",
    "    return len(input_fields)\n",
    "\n",
    "def images_amount(url):\n",
    "    \n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    images = soup.find_all('img')\n",
    "    \n",
    "    return len(images)\n",
    "\n",
    "def iframes_amount(url):\n",
    "    \n",
    "    page = urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    iframes = soup.find_all('iframe')\n",
    "    \n",
    "    return len(iframes)\n",
    "\n",
    "def content_features(url):\n",
    "    \n",
    "    if url != None:\n",
    "\n",
    "        title_terms_count = title_terms_amount(url)      \n",
    "        body_terms_count = body_terms_amount(url)\n",
    "        inputs_count = input_field_amount(url)\n",
    "        images_count = images_amount(url)\n",
    "        iframes_count = iframes_amount(url)    \n",
    "      \n",
    "        return (title_terms_count, body_terms_count, inputs_count, images_count, iframes_count)\n",
    "    \n",
    "print(content_features(example_urls[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iframes_amount('https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urlopen('https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_iframe').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "iframes = soup.find_all('iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<iframe src=\"https://www.w3schools.com\">\n",
       "   <p>Your browser does not support iframes.</p>\n",
       " </iframe>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Notes\n",
    "\n",
    "[Know Your Phish: Novel Techniques for Detecting\n",
    "Phishing Sites and their Targets](https://arxiv.org/pdf/1510.06501.pdf)\n",
    "\n",
    "Modeling phisher limitations: To increase their chances\n",
    "of success, phishers try to make their phish mimic its\n",
    "target closely and obscure any signal that might tip off the\n",
    "victim. However, in crafting the structure of the phishing\n",
    "webpage, phishers are restricted in two significant ways.\n",
    "First, external hyperlinks in the phishing webpage, especially those pointing to the target, are to domains outside\n",
    "the control of phishers. \n",
    "\n",
    "Second, while phishers can freely\n",
    "change most parts of the phishing page, the latter part\n",
    "of its domain name is constrained as they are limited\n",
    "to domains that the phishers control. We conjecture that\n",
    "by modeling these limitations in our phishing detection\n",
    "classifier, we can improve its effectiveness.\n",
    "\n",
    "Measuring consistency in term usage: A webpage can\n",
    "be represented by a collection of key terms that occur\n",
    "in multiple parts of the page such as its body text, title,\n",
    "domain name, other parts of the URL etc. We conjecture\n",
    "that the way in which these terms are used in different\n",
    "parts of the page will be different in legitimate and\n",
    "phishing webpages.\n",
    "\n",
    "\n",
    "A phisher\n",
    "has full control over the subdomains portion and can set it to\n",
    "any value. The RDN portion is constrained since it has to be\n",
    "registered with a domain name registrar\n",
    "\n",
    "useful features Starting URL, Landing URL,Redirection chain,Logged links,HTML\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[DeltaPhish: Detecting Phishing Webpages\n",
    "in Compromised Websites](https://arxiv.org/pdf/1707.00317.pdf)\n",
    "\n",
    "\n",
    "\n",
    "[PhishAri: Automatic Realtime Phishing Detection on Twitter](https://arxiv.org/pdf/1301.6899.pdf)\n",
    "\n",
    "\n",
    "\n",
    "[More or Less? Predict the Social Influence of Malicious URLs on Social Media\n",
    "](https://arxiv.org/abs/1812.02978)\n",
    "\n",
    "\n",
    "\n",
    "[awesome-threat-intelligence](https://github.com/hslatman/awesome-threat-intelligence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
